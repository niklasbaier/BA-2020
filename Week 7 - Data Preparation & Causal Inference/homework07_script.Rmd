---
title: "Homework 7 - Analytics Case Study"
author: Â© TUM - Decision Sciences and Systems (I18)
output:
  html_document:
    df_print: paged
    toc: true
    toc_float: true
    toc_depth: 3
#editor_options: 
  #chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

## What is this?

This 'homework sheet' contains a small case study in data modeling that is meant to prepare you for the analytics cup. In particular, it will demonstrate some additional functionality in R which may come handy when you analyse your own data set, in particular with regards to data exploration and meta machine learning.

Except for methods and concepts which is covered in the Week 7 Lecutre and Tutorials and in other weeks of the class, the functionality shown in this file is *not* relevant to the end-term exam; it is merely meant to give you useful tools for practical data analysis.

There are _no exercises_ that you need to solve in the homework, but you should work through this file in R and try to understand the individual steps performed.

## How is this rleated to the Analytics Cup?

In the Analytics Cup, you will receive a few files including

* some data
* some background information about the data
* clear instructions on what you should do
* a test set (without labels), or instructions which give you enough information to create the test set yourself.

You should use this file to understand what a typical analytics workflow looks like. During the AC,
you may want to use this file as a reference to get started or to look up useful functions and packages. In particular, we demonstrate
* How to make some exploratory plots using `ggplot2`.
* How the meta-machine learning package `tidymodels` can be used to apply many different R-machine learning packages with a common interface.


## What file format is this?

This file is an _R-markdown_ (`.Rmd`) notebook, which can be used to make presentations and reports containing R-Code,
output generated by that code and text. In RStudio, you can use the `Knit` function (button in top toolbar) to compile it to an .html or .pdf report.
When you run an R-markdown file inside RStudio, results of each code-block (including plots, interactive widgets etc) will be shown inline underneath the code, rather than in a separate console.

**Note**: _In the analytics cup, you will have to submit the script that generated your submission. This should be a regular R script, not a R-Notebook.


## What this is not

* This should _NOT_ be considered a 'perfect' analysis of the given challenge. Some important but menial steps have been ommitted here, e.g. re-examining and iterating on data preparation after the first model results.
* This is _NOT_ a template script for the Analytics Cup in January 2021. You will receive a different challenge and different dataset, so you cannot just replace the data loading with your own and excpect a valid result.
* This is _NOT_ a complete overview of useful R tooling, especially we only scratch the surface of what meta machine learning packages have to offer. You are advised to read many of the tutorials found at https://tidymodels.org 

## How is this document structured

First, we will describe the challenge for the case study and the files you would be provided in this case. After that, we will go through the entire workflow once, highlighting useful features of different packages that you may want to use. In particular we

* Load the Data
* Explore and clean the data
* Load an additional data source and join it to the main data frame
* Clean the data and prepare it for modeling
* Create a model setup (workflow, recipe and model prototype) in `tidymodels`
* Validate the model using cross validation and evaluate the results in `tidymodels`
* Train the final model and make predictions on the test set
* Create the submission file that matches the template submission.

# The challenge


The dataset in this case study is based on the Global Power Plant Database commissioned by the World Resources Institute: http://datasets.wri.org/dataset/globalpowerplantdatabase. (This is the same data that was used in exercise 7.2 of the tutorials.)

The data contains information about power production of a given power plant in each year. The goal of this analysis is the following:
_Predict the power generation of a plant in 2017, wherever this data in missing in the original data set._

_Citation_:

> Global Energy Observatory, Google, KTH Royal Institute of Technology in Stockholm, Enipedia, World Resources Institute. 2018. Global Power Plant Database. Published on Resource Watch and Google Earth Engine; http://resourcewatch.org/ https://earthengine.google.com/

_GitHub_:
>https://github.com/wri/global-power-plant-database

## Files that would be included in the Analytics Cup
* `README.md` a file that clearly outlines your task. (omitted in this homework case study)
* `global_power_plants.csv` - the data
* `fuel_generation_by_country.csv` - additional data for feature generation.
* `data_info.txt` - some background information about the data (omitted in this homework case study)
* `case_study_submission_template.csv` - the test set and/or a template that specifies the format of a valid submission.



# Loading packages and data

We start by loading the packages required for analysis.
(Please note that when we use the `parsnips` package below, it will try to load additional packages in the background, which need to be installed, but not explicitly loaded by the user.)


```{r echo = TRUE, message=FALSE}
# data preprocessing
library(tidyverse)
library(lubridate)
# data exploration
library(summarytools) # for user-friendly html summaries of data
library(ggmap) # for plotting data on a map
# for meta-ml
library(tidymodels)

# let's set some global options
options(dplyr.width = Inf) # show all columns when printing to console
theme_set(theme_minimal()) # set ggplot theme for cleaner plotting
set.seed(2021) # in the AC, you'll be required to set a fixed random seed to make your work reproducible
```


For this homework case study, we'll use the sama data as in the tutorial exercise 7.2. We'll read the data and perform the same data cleaning steps as in the tutorial exercise.


```{r}
plants <- read_csv(
  'global_power_plants.csv',
  col_types = cols(
    other_fuel3 = col_character(),
    wepp_id = col_character(),
    production_gwh = col_double()
  )
)

countries <-  read_csv(
  'fuel_generation_by_country.csv') %>%
  pivot_longer(cols = -country,
               names_to = "fuel",
               values_to = "generation_gwh") %>%
  mutate(generation_year = 2014)

countries[
  countries$country=='Niger' & countries$fuel=='Total',
  ]$generation_gwh <- 690
```

In the html-output, the following two blocks will provide interactive widgets that let you explore the tables visually.:

```{r}
plants
```


```{r}
countries
```


# Exploratory Analysis and Preprocessing


The `summarytools` package contains a useful tool to quickly give you a report of your data:

**Note:**  _The output won't be properly displayed if you're viewing the `.html`-report, run the source code yourself to open the summary-report in a browser or the RStudio `Viewer` tab._

```{r}
plants %>% dfSummary %>% view(
  #method = 'browser' #enable this to use an external webbrowser to view the output
)
```



There's some columns we definitely won't need in this analysis (see data_info.txt), let's drop them now.
```{r}
plants <- plants %>% select(-source, -url, -geolocation_source, -wepp_id,)
plants %>% sample_n(10)
```

Next, let's fix the data in `commissioning_year` - the column contains decimal numbers, not just full years.

```{r}
plants <- plants %>% 
  mutate(
    commissioning_year = as_date(date_decimal(commissioning_year))
  ) %>% 
  rename(
    id = gppd_idnr,
    commission_date = commissioning_year
  )
```


## Some exploratory plots

In this section we will use the `ggplot2` package to make some plots that may help us understand the data better. 



### Number of Power Plants by Country

```{r}
plants %>% 
  # only use countries with at least 80 entries
  group_by(country) %>%
  filter(n() > 80) %>%
  ungroup() %>%
  ggplot( aes(x=country)) +
  geom_bar() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


### Power Plants by type

```{r}
plants %>% ggplot(aes(x=primary_fuel, fill=primary_fuel)) +
  geom_bar()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


### Total capacity by type

```{r}
plants %>% ggplot(aes(y=capacity_mw, x= primary_fuel, fill = primary_fuel)) +
  geom_boxplot() +
  scale_y_log10() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```



### Fuel mix in top 10 countries by total capacity

```{r}
countries_by_capacity <- plants %>% 
  group_by(country) %>%
  summarize(cap = sum(capacity_mw))
top_10_countries <- countries_by_capacity %>%
  top_n(10, cap) %>% pull(country)
plants %>%
  filter(country %in% top_10_countries) %>%
  ggplot(aes(x=country, y=capacity_mw, fill=primary_fuel)) +
  geom_bar(stat='identity') +
  scale_fill_viridis_d()
```


### Fuel mix in top countries, stacked to 100%

```{r}
plants %>%  filter(country %in% top_10_countries) %>% ggplot(aes(x=country, y=capacity_mw, fill=primary_fuel)) +
  geom_bar(stat='identity', position='fill') +
  scale_y_continuous(labels= scales::percent_format()) +
  scale_fill_viridis_d()
```

### Optional exercise if you want to learn ggplot:
For all nuclear power plants in the U.S., make a line-chart of the development of annual power generation.

Hint: Use `tidyr::pivot_longer` for data preparation to get separate variables 'observation_year' and 'observation_value' per power station, then ggplot with `geom_line` using `aes(..., group=name)`.

### Capacity vs generation

```{r, fig.height=8, warning=F}
my_plot <- plants %>% 
  # disregard missing values when making the plot scales
  filter(generation_year==2017, !is.na(production_gwh)) %>% 
  ggplot(aes(x=capacity_mw, y=production_gwh, col = year(commission_date), 
             # we can define extra aesthetics that we won't use (see below)
             label=name, label2=country)) +
  geom_point() +
  # plot line indicating full capacity being used (i.e. GWh = MW/1000 * 24*365.25 h)
  stat_function(fun = (function(x) 24*365.25/1000*x), col='grey') +
  facet_wrap(~primary_fuel, scales='free') +
  theme(#legend.position = 'None',
        axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = 'bottom') +
  scale_color_viridis_c()
my_plot
```


#### Tip: Making a Plot interactive Using `plotly`

```{r, warning=F}
# requires installation of package 'plotly'
## Note: often it's useful to not `load` this package with library
## because it will mask many functions from other packages
## instead we call the functionality with plotly:: without loading the entire package
plotly::ggplotly(my_plot, tooltip = c('x', 'y','label', 'label2', 'col'))
```


# Modeling the data

Our goal is to estimate power generation in 2017 whenever that data is missing for a plant, so our observations for this analysis will be comprised by power plants. Data from earlier years may, in fact, be useful predictiors and we would like to use them as features. In this case, it will be useful to put the original data (observation = plant,year) into wide format regarding the generated output.

Let's create a dataframe `df` which will serve as our main data structure to hold the data as we will model it:

```{r}
df <- plants %>% pivot_wider(
  names_from = generation_year,
  names_prefix = 'generation_gwh_',
  values_from = production_gwh
)
df
```

Now that we have `df` in the format such that each row is one powerplant, we can separate the training set (data for 2017 is present) and the test set:

```{r}
train <- df %>% filter(!is.na(generation_gwh_2017))
test <- df %>% anti_join(train)
```

At this point, in the Analytics Cup, you should check whether your test set matches the submission template.

```{r}
submission_template <- read_csv('case_study_submission_template.csv')
template_ids <- submission_template %>% arrange(id) %>% pull(id)
test_ids <- test %>% arrange(id) %>% pull(id)
all(template_ids == test_ids)
```

Looking good!


## Do train and test data look the same?

You should always check this - if there are big differences 
* you might not be able to train a model at all (e.g. different columns, different factor-levels)
* your model will necessarily be biased if the distribution of features are different.

### Plotting data on a map

The package `ggmap` offers some nice functionality for plotting geographical data.
The easiest way is `ggmap::qmap('world')` , `qmap('Germany')`, `qmap('Tokyo')` etc, to download
map data, but you will need a personal Google Maps API key to use qmap. See `?qmap` for details.
Note: Make sure not to download too many tiles per month from google or you will incurr Google Cloud charges!

Instead, we will use open source map material from Stamen here, which has unlimited free calls but
requires manual configuration of the plotted area.


```{r, fig.width=10}
left = -180
right = 180
bottom = -60# min(df$latitude)
top = max(df$latitude)
bbox = c(left=left, bottom=bottom, right=right, top=top)

world <- ggmap::get_stamenmap(bbox, zoom = 2, maptype = 'terrain-background')
# see ??get_map for more options
ggmap(world)
```


We can now use our downloaded world map and plot things on top of it. 

### Geographical Location of training and test set instances

```{r, fig.width=10}
map_data <- df %>% mutate(set = if_else(id %in% train$id, 'train', 'test'))

ggmap(world, ggplot()) +
  geom_point(aes(x=longitude, y=latitude, col=set), size=0.5, data=map_data) +
  ylim(-60, top) +
  theme(legend.position = 'top')
```



### Comparing Summaries of Train and Test set


```{r}
train %>% dfSummary %>% view
```

```{r}
test %>% dfSummary %>% view
```

Training on this data is problematic

* Good quality training data only comes from a handful of countries --> cannot use country as a feature directly
* Unclear how other features will translate to other countries
* Most test set observations also don't have data for the previous years
* Prediction is essentially down to capacity, fuel_type and age of the plant


## Enriching Data using External Sources

The country_data table contains total generation per country, so let's use that data to enrich `df`.

Using this, we will build additional features: 

* Plant capacity as share of country's total power generation
* Plant capacity as share of coutnry's total power generation of same fuel type

(You have already done this in exercise 7.2, so we will skip the details and just create the features in a single block.)

```{r}
df <- df %>%
  left_join(
    # we omitt "2014" from the name for brevity
    countries %>% rename(country_gen_by_fuel=generation_gwh),
    by=c("country_long" = "country", "primary_fuel" = "fuel")
    ) %>% 
  left_join(
    countries %>% filter(fuel == 'Total') %>% transmute(country, country_gen_total = generation_gwh),
    by = c("country_long" = "country")
  )

df <- df %>% 
  mutate(
    country_gen_by_fuel = 1000/24/365.25 * country_gen_by_fuel,
    country_gen_total   = 1000/24/365.25 * country_gen_total
  ) %>% 
  mutate(cap_share_of_country_gen_by_fuel = capacity_mw/country_gen_by_fuel,
         cap_share_of_country_gen_total = capacity_mw / country_gen_total)
```

At this point, we should check whether we missed certain entries (countries or fuels,...), or we could go back and e.g. change all 'Wave and Tidal' plants to 'Hydro' if you believe that makes sense or take additional steps to augment our training data.
Here we will skip these improvements.

## Even more preprocessing...


```{r}
df %>% summary
```


Our last operation introduced `Inf`inite values, which many models may not be compatible with. Let's fix them ourselves
(Exercise: Here we arbitrarily choose 1.0, is that a good choice?)

```{r}
df <- df %>% mutate(
  cap_share_of_country_gen_by_fuel = if_else(is.infinite(cap_share_of_country_gen_by_fuel), 1.0, cap_share_of_country_gen_by_fuel)
)
```

Let's get rid of all columns that we will not use in our model.
(Again, this might not be the best choice, it's just an example.)
```{r}
names(df)

```

```{r}
df <- df %>% select(-country, -country_long, -name, -latitude, -longitude,
                     -other_fuel2, -other_fuel3, -owner,
                     -generation_gwh_2013, -generation_gwh_2014, -generation_gwh_2015, -generation_gwh_2016,
                     -country_gen_by_fuel, -country_gen_total)
```

We have changed our data quite a bit in these additional feature engineering steps, so let's replace the `train` and `test` sets with an updated version:


```{r}
train <- df %>% filter(!is.na(generation_gwh_2017))
test <- df %>% anti_join(train) %>% select(-generation_gwh_2017)
```

```{r}
dfSummary(train) %>% summarytools::view()
```

```{r}
dfSummary(test) %>% summarytools::view()
```


## Modeling using Tidymodels

Let's first set up a `recipe` that will specify additional processing steps (which we might want to iterate on), such as imputation, discretization etc.

For this, we can use the `recipes` package (`??recipes`) and its functionality.

### Specifying a preprocessing recipe

```{r}
rec <- recipe(
  #specift predictors, target and data to enable code autocompletion
  generation_gwh_2017 ~ ., data = train) %>% 
  # tell tidymodels that `id` is an ID and should not be used in any model
  update_role(id, new_role = "ID") %>% 
  # turn dates into decimals, e.g. 2019-07-01 becomes 2019.5
  step_mutate_at(where(is.Date), fn=decimal_date) %>% 
  step_mutate_at(cap_share_of_country_gen_by_fuel, cap_share_of_country_gen_total,
                 # this is a shorthand notation for (function(x) {replace_na(x,0)})
                 fn= ~replace_na(.,0)) %>%
  # impute all other numeric columns with their mean
  step_meanimpute(all_numeric()) %>% 
    # determine what happens when a new nominal value is encountered in test data (which was missing from the trianing set)
  step_novel(all_nominal(), -has_role("ID"), new_level="new") %>% 
  # impute all other nominal (character + factor) columns with the value "none"
  step_unknown(all_nominal(), new_level = "none") %>% 

  # convert all strings to factors
  step_string2factor(all_nominal(), -all_outcomes(), -has_role("ID")) %>% 
  # remove constant columns
  step_zv(all_predictors())
```

We now have a recipe specification.
```{r}
rec
```

"Preparing" the recipe via `prep` will now determine any necessary information from the data in `train`, e.g. finding the means for mean-imputed columns, determining the levels which appear in categorical columns etc.

```{r}
rec %>% prep()
```

We can apply ("bake") a prepped recipe to any dataset which matches the specification of train, e.g. our test set. For example, missing values in numeric columns in the test set would be imputed by the mean of the *training* (!) set.

```{r}
rec %>% prep() %>% bake(test)
```

### Resampling

However, note that we didn't save the outcome of prepping and baking the recipe. This is because we will want to use resampling in order to better evaluate the performance of our model (compare Week 8 Lecture and Exercises). To do so, we rely on functionality in the `rsample` package. In this case, we will use 5-fold cross-validation, using `rsample::vfold_cv`.

```{r}
folds <- train %>% vfold_cv(v=5)
```

### Model Specification

Now, let's specify a model that we want to train. In this case,
let us train a random forest using the `ranger` package. We want to predict a ratio-scaled variable, so we'll need a random forest regression (rather than classification) model. We can specify this model using the `parsnips` wrapper around the `ranger` backend:

```{r}
model <- rand_forest(
  mode = 'regression',
  # you could set additional hyperparameters here if desired
  ) %>% set_engine("ranger",
                   # you could set additional ranger-specific params here
                   importance = "impurity" #needed for feature importance plot below
                   )

model
```

### Training Workflow

Finally, let's set up our entire modeling workflow:

```{r}
training_workflow <- 
  # start with a blank workflow() object
  workflow() %>% 
  # add our preprocessing recipe
  add_recipe(rec) %>% 
  # add our model specification
  add_model(model)

training_workflow
```

We could now apply this training workflow to any data that looks like our trianing set. Here we will use it for all folds in cross validation, to understand expected model performance on the test set. We need to specify which evaluation metrics we will be interested in. In our case, let's use the rmse (Root Mean Squared Error) and mae (Mean Absolute Error). Which metrics matter will be determined by the challenge instructions, or your intermediate analysis goals. Most common metrics are available in the `yardstick` package, and you can write your own custom metrics if needed.

(Training the 5 models may take a while, depending on your system.)


```{r}
cv_fits <- 
  training_workflow %>% 
  fit_resamples(folds,
                metrics = metric_set(yardstick::rmse, yardstick::mae)
                )
```

### Evaluating and Tuning

We can collect the resampling result via 
```{r}
cv_fits %>% collect_metrics()
```

in this case, we observe a RMSE of 773 and a MAE of 230 across the 5 folds. At this point, we might go back and try different models and preprocessing recipes in order to improve performance. (Omitted in this document.) 

We may also want to tune hyperparameters in a given model class. For some automated functionality regarding this, have a look at the `tune` and `dial` packages in tiymodels.

`fit_resamples` also offers you functionality to try multiple recipes and model specifications at the same time. (See the tutorials on tidymodels.org to find out how.) Let's say we have done this and want to select our best combination for final training and predicting on the test set. (This is pointless in this example, because we only tried a single configuration.)

```{r}
best_config <- cv_fits %>%
  # find the best tried configuration for a certain criterion
  select_best('rmse')
final_workflow <- training_workflow %>% 
  finalize_workflow(best_config)

final_workflow
```

## Training the final model and validating it
Now let's train our best candidate on the entire training set:

```{r}
trained_model <- final_workflow %>% 
  fit(data=train)

trained_model
```

Let's apply the model to the training set itself and see how it did. (Exercise: Why is this not a good way to determine the expected model performance?)

```{r}
train_set_with_predictions <-
  bind_cols(
    train,
    trained_model %>% predict(train)
  )
train_set_with_predictions
```
We see that a new column `.pred` has been added to the training set. Let's compare it to the true labels:

```{r}
train_set_with_predictions %>% ggplot(aes(x=generation_gwh_2017, y=.pred)) +
  geom_point() +
  stat_function(fun=identity)
```

The correlation between predictions and true labels is high. The model seems to work quite well, at least with previously seen data.

Depending on your task (regression/classification) and model, there may be additional evaluation concepts you may want to use. For a random forest, we might want to look at calculated feature importance. We can use the `vip` package ("variable importance") for this:

```{r}
 trained_model %>% pull_workflow_fit() %>% vip::vip()
```


In this case, the capacity of a power plant seems to be the best single predictor of its power output in 2017.

## Predicting on Test set
Once we're happy with our model, we can create predictions on the test set to make a submission:

```{r}
submission <-bind_cols(
  test %>% select(id),
  trained_model %>% predict(test)
) %>% 
  # column names must be the same as in submission template!
  rename(prediction = .pred) %>% 
  # order by id
  arrange(id)
```


And now we have a valid submission.
Important: for the real analytics cup, you should save your submission with a descriptive name. You are allowed to make 10 submissions in total and any mismatch between submission file and script will be YOUR responsibility and could lead to disqualification.

```{r}
write_csv(submission, "TEAMNAME_SUBMISSION_NUMBER_5.csv")
```


